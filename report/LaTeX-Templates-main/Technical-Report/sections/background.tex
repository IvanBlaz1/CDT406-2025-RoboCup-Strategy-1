\section{Background}

\subsection{Autonomous Mobile Robots}
An (Autonomous) Mobile Robot is a robot that is capable of moving around and navigating through its
surroundings with the help of for example software, sensors and cameras. The robots
are mainly fitted with legs, wheels or tracks that are used to transport itself around, but
they are also used in aerial and nautical environments. They are mainly driven by an
automated AI system that is in charge of decision-making. Mobile robots have surged in
popularity over the recent years (partly) due to their ability to operate in areas that
humans can not/should not be in\cite{KateBrush1Robots}.
%sdf

\subsection{Behavior Trees}
Behaviour trees (BT) are a way to structure the switching between different tasks in autonomous agents. This kind 
of structure was developed for controlling NPCs (non-player characters) in games and they are both modular and reactive. 
Modular meaning that the system consists of components that are independent and reusable, e.i. the components can be 
tested individually or removed without changing the whole tree. Reactive, on the other hand, means that the system 
adapts to changes in the surrounding space and can for example change its behaviour based on what is happening. The 
structure of a BT resembles a directed rooted tree with internal nodes called control flow nodes and leaf nodes called 
execution nodes. Each connected node are most often called parent and child where the root is the node without parents. 
The execution of the tree starts with the root that sends signals with a given frequency to its children to start executing 
and these signals are called ticks. Depending on what type a child node is, it will work differently. There are typically 
four types of control flow nodes and two types of execution nodes. Starting with the control flow nodes:
\begin{itemize}
    \item \textbf{Sequence}: routs the ticks to its own children (starting from the left) and returns failure or running to its own parent when one of its children returns failure or running. Only when it has passed all its children and every child has returned success, it returns success to its own parent.
    \item \textbf{Fallback}: (also called selector) works almost like a reversed version of sequence. It routs from left to right and returns success or running as long as one child returns it and returns failure only if all children return failure. 
    \item \textbf{Parallel}: works differently from the other two, routing the ticks to all its children which means all children execute simultaneusly. The user defines how many children that needs to return succcess for the parallel node to return success to its parent. It returns failure if \textit{N-M+1} children return failure where \textit{M} is the user-defined amount and \textit{N} is the number of cildren.
    \item \textbf{Decorator}: only has one child. It is used to check that child with special conditions, such as limited execution time, or inverting the status of that child. 
\end{itemize}
For graphical representation, the sequence node is represented by a "$\rightarrow$" sign, the fallback node by a "?" sign, the parallel node by a "$\rightrightarrows$" sign, all of them with a box around the specific sign. Lastly, the decorator is represented by a rhombus.
Continuing with the execution nodes:
\begin{itemize}
    \item \textbf{Action}: executes its task and returns running while executing and success or failure when completed.
    \item \textbf{Condition}: checks a condition and returns either success (if its true) or failure (if its false) and never returns running. 
\end{itemize} 
The execution nodes are represented by a rectangle for the action node and a circle for the condition node with the specific action/condition written inside it.
In this way the flow of tasks can be controlled which makes BTs a useful tool when developing an AI system\cite{BTsinAI}. 

\subsection{Genetic Algorithm}
Genetic algorithm is a machine learning algorithm inspired by the theory of natural selection. The goal of the algorithm is to find the best possible solution to a given problem by following an evolutionary process. A population of possible solutions are tested and evaluated to measure their fitness, which is how good they are at solving the given problem. The best solutions are then brought into the next generation of solutions for further testing. The best solutions get to mate with other fitting solutions to create offspring that will have received traits from their parents. The solutions will also receive mutations to maintain genetic diversity. This entire process is then repeated until a satisfying or the best possible solution is found\cite{BiscontiniGA}.

\subsection{Reinforcement Learning}
Reinforcement learning or RL for short, is a machine learning algorithm that trains an AI model to independently make its own choices. The model learns to find the optimal policy by interacting with its environment by taking actions to find the best value function for a given policy. Through continuous training iterations it will find the best policy in regards to its value function, and in the end find the best possible solution \cite{ieeeRL1}. (BÖRJA MED STANFORD HÄRIFRÅN)
There are three main elements and an additional optional one in reinforcement learning. These are policy, reward signal, value function and optionally a model. The policy is the current behavior of the agent. It decides on what action to take depending on the current state. The reward signal, or more generally reward, is the immediate reward that the model is given for performing a certain action. The value of the reward helps the model determine if an action was good or bad. The value function helps the model determine the best choice to take currently based on what its future rewards will look like by taking said action from its current state. The value function calculates what the best possible actions will be, to maximize its reward in the long run. The fourth and optional element is the model, but they are optional. A model-based method is based around predicting the future reward without having visited each state first, and a model-free method is based around the model learning only by itself by exploring through trial and error.


\subsection{Deep Reinforcement Learning}
Given a finite amount of actions and states, Q-learning can, therefore, learn the optimal action to take at each state to ensure the maximum total reward according to some time horizon. In 2013, Mnih\cite{DQN} proposed a variant of Q-learning called Deep Q Network (DQN), in which a neural network is used to approximate the optimal action-value function
\[
    Q^*(s,a)=\underset \pi {\text{max}} \mathbb{E}\left[ r_i+\gamma r_{t+1}+\gamma^2 r_{t+2}+\cdots \,\middle|\, s_t=s,\,a_t=a,\,\pi \right]
\]
(which is the maximum sum of rewards \(r_t\) discounted by the time horizon \(\gamma\) at each timestep \(t\), achievable by a behavior policy \(\pi=P(a|s)\), after making an observation \(s\) and taking an action \(a\)), by means of gradient decent of the loss function
\[
L_i(\theta_i)=\mathbb{E}_{(s,a,r,s')~U(D)}\left[\left(r+ \gamma \underset {a'} {\text{max}} Q(s',a';\theta^-_i)-Q(s,a;\theta)\right)^2\right]
,\]
where the quaternion \((s,a,r,s')\) represents a so-called ``experience replay'' of a past action \(a\) at a certain state \(s\), the received reward \(r\) and the next state\(s'\) following the action. With this method, then -- unlike with regular Q-learning -- an action policy for a continuous state space (like in the scenario of soccer robots in a simulation) can be learned.

\subsection{Proximal Policy Optimization (PPO)}
Proximal Policy Optimization (PPO) is a model-free reinforcement learning algorithm within the family of policy-gradient techniques. 
It enhances training stability through policy update clipping to prevent detrimental changes between iterations \cite{Schulman2017PPO}. 
PPO is broadly considered a simple, yet effective baseline for continuous control and multi-agent environments. 
As outlined in OpenAI's Spinning Up guide, PPO aims to offer a "reliable and stable improvement step for policy gradient methods" \cite{OpenAISpinningUp}. 
While PPO was originally proposed as a single-agent method, it has been widely adapted for multi-agent reinforcement learning. 
In particular, using a centralized critic with decentralized policies (often referred to as MAPPO) has become a standard approach in domains such as robotics and multi-agent games \cite{Yu2021MAPPO}. 
Its efficiency, robustness, and ease of use make PPO an attractive candidate for RoboCup SSL, wherein agents are required to act in continuous spaces and coordinate in dynamic multi-agent settings.

\subsection{VMAS}
VMAS (Vectorized Multi-Agent Simulator) is a framework designed for benchmarking multi-agent reinforcement learning. It features a fully differentiable, vectorized 2D physics engine implemented in PyTorch and supports the development of custom multi-agent scenarios \cite{bettini2022vmas}. In this project, a new simulation model was created to replicate a football match in the B Division of the Small Size League. The virtual environment included a field and agent-based robots, all scaled to real-world dimensions. Core game mechanics—such as out-of-bounds, goal kicks, and corner kicks—were implemented. Additionally, fundamental skills, including shooting, passing, positioning, dribbling, creating space, and throw-ins, were incorporated.

\subsection{Other teams}
The CMDragons team won all six games they played during the RoboCup 2015 competition. In this paper they 
describe how they used simpler algorithms to divide their robots into defense and offense subteams to suit the state 
of the game. They switched between the amount of robots depending on parameters such as ball possession, 
field region and the aggressivness of the other team. In offense, they used algorithms to both estimate the 
optimal place to move for robots without the ball as well as the best action for the robot in possession of the 
ball. In defensive situations, algorithms were used to evaluate the threats. Both first-level and second-level 
threats were computed in order to stop the robot with the ball to score directly and to stop threatening passing 
options. Using these methods, the CMDragons were able to win the competition without conceding a single goal\cite{CMDragons2015}.
Due to there succes, it can be useful to investigate if certain skills are more efficient to implement using simpler
types of algorithms instead of using AI models. 
