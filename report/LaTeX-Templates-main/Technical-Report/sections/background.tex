\section{Background}

\textbf{Autonomous Mobile Robots}:
According to Kate Brush \cite{KateBrush1Robots}, an (Autonomous) Mobile
Robot is a robot that is capable of moving around and navigating through its
surroundings with the help of for example software, sensors and cameras. The robots
are mainly fitted with legs, wheels or tracks that are used to transport itself around, but
they are also used in aerial and nautical environments. They are mainly driven by an
automated AI system that is in charge of decision making. Mobile robots have surged in
popularity over the recent years (partly) due to their ability to operate in areas that
humans can not/should not be in.


\textbf{Reinforcement Learning}:
Reinforcement learning: According to Jacob Murel and Eda Kavlakoglu \cite{JacobMurel1RL}, reinforcement learning is a machine learning
algorithm that is used to develop independent decision making in autonomous agents.
Agents train by repeating similar tasks over a period of time or repetitions, where they
learn independently through trial and error. A popular adaptation/version of the learning
algorithm is Q-learning. According to GeeksForGeeks \cite{GeeksForGeeks1RL}, Q-learning is a model-free RL
algorithm that is used for training independent agents to make the best decision
possible in each possible situation. It learns through a trial and error system, where it
interacts with the environment to find the best method. A state-action-reward system is
utilized, where the result of an action taken in a state is rewarded or penalized
depending on the outcome. After a training iteration it writes its Q-values to a Q-table,
where the values represent the best known expected reward for taking a given action in
a given state. It updates the table using the Temporal Difference rule:
\(Q(S, A) \leftarrow Q(S, A) + \alpha \left( R + \gamma Q(S', A') - Q(S, A) \right)\). For each state, the agent can either choose to
explore or to exploit. Using the Epsilon-Greedy Policy (\(\epsilon\)-greedy policy), the agent
decides whether to take the best current known action (exploit), where the agent picks
the best action with the highest Q-value based on the probability of \(1-\epsilon\). Else it will try to
find a new best possible action (explore), where the probability to explore is based
simply on the \(\epsilon\)-value. This is what allows the model to independently over time find the
best possible outcomes for each state. 


Q-learning is a reinforcement learning algorithm used for leaning the value of actions in states according to some reward definition. Given a finite amount of actions and states, the algoritm can learn by experience the optimal action to take at each state to ensure the maximum total reward according to some time horizon.

In 2013, Mnih [...] proposed a variant of Q-learning called Deep Q Network, in which a neural network is used to aproximate the optimal action-value function
\begin{equation}
    Q^*(s,a)=\underset \pi {\text{max}} \mathbb{E}\left[ r_i+\gamma r_{t+1}+\gamma^2 r_{t+2}+\cdots \,\middle|\, s_t=s,\,a_t=a,\,\pi \right]
\end{equation}



\textbf{Q-Learning}: