\section{Background}

\subsection{Autonomous Mobile Robots}
An (Autonomous) Mobile Robot is a robot that is capable of moving around and navigating through its
surroundings with the help of for example software, sensors and cameras. The robots
are mainly fitted with legs, wheels or tracks that are used to transport itself around, but
they are also used in aerial and nautical environments. They are mainly driven by an
automated AI system that is in charge of decision-making. Mobile robots have surged in
popularity over the recent years (partly) due to their ability to operate in areas that
humans can not/should not be in\cite{KateBrush1Robots}.
%sdf

\subsection{Behavior Trees}
Behaviour trees (BT) are a way to structure the switching between different tasks in autonomous agents. This kind 
of structure was developed for controlling NPCs (non-player characters) in games and they are both modular and reactive. 
Modular meaning that the system consists of components that are independent and reusable, e.i. the components can be 
tested individually or removed without changing the whole tree. Reactive, on the other hand, means that the system 
adapts to changes in the surrounding space and can for example change its behaviour based on what is happening. The 
structure of a BT resembles a directed rooted tree with internal nodes called control flow nodes and leaf nodes called 
execution nodes. Each connected node are most often called parent and child where the root is the node without parents. 
The execution of the tree starts with the root that sends signals with a given frequency to its children to start executing 
and these signals are called ticks. Depending on what type a child node is, it will work differently. There are typically 
four types of control flow nodes and two types of execution nodes. Starting with the control flow nodes:
\begin{itemize}
    \item \textbf{Sequence}: routs the ticks to its own children (starting from the left) and returns failure or running to its own parent when one of its children returns failure or running. Only when it has passed all its children and every child has returned success, it returns success to its own parent.
    \item \textbf{Fallback}: (also called selector) works almost like a reversed version of sequence. It routs from left to right and returns success or running as long as one child returns it and returns failure only if all children return failure. 
    \item \textbf{Parallel}: works differently from the other two, routing the ticks to all its children which means all children execute simultaneusly. The user defines how many children that needs to return succcess for the parallel node to return success to its parent. It returns failure if \textit{N-M+1} children return failure where \textit{M} is the user-defined amount and \textit{N} is the number of cildren.
    \item \textbf{Decorator}: only has one child. It is used to check that child with special conditions, such as limited execution time, or inverting the status of that child. 
\end{itemize}
For graphical representation, the sequence node is represented by a "$\rightarrow$" sign, the fallback node by a "?" sign, the parallel node by a "$\rightrightarrows$" sign, all of them with a box around the specific sign. Lastly, the decorator is represented by a rhombus.
Continuing with the execution nodes:
\begin{itemize}
    \item \textbf{Action}: executes its task and returns running while executing and success or failure when completed.
    \item \textbf{Condition}: checks a condition and returns either success (if its true) or failure (if its false) and never returns running. 
\end{itemize} 
The execution nodes are represented by a rectangle for the action node and a circle for the condition node with the specific action/condition written inside it.
In this way the flow of tasks can be controlled which makes BTs a useful tool when developing an AI system\cite{BTsinAI}. 

\subsection{Genetic Algorithm}
Genetic algorithm is a machine learning algorithm inspired by the theory of natural selection. The goal of the algorithm is to find the best possible solution to a given problem by following an evolutionary process. A population of possible solutions are tested and evaluated to measure their fitness, which is how good they are at solving the given problem. The best solutions are then brought into the next generation of solutions for further testing. The best solutions get to mate with other fitting solutions to create offspring that will have received traits from their parents. The solutions will also receive mutations to maintain genetic diversity. This entire process is then repeated until a satisfying or the best possible solution is found\cite{BiscontiniGA}.

\subsection{Reinforcement Learning}
Reinforcement learning (RL) is a machine learning
algorithm that is used to develop independent decision-making in autonomous agents.
Agents train by repeating similar tasks over a period of time or repetitions, where they
learn independently through trial and error. A popular implementation of the learning
algorithm is Q-learning\cite{JacobMurel1RL}. Q-learning is a model-free RL
algorithm that is used for training independent agents to make the best decision
possible in each possible situation. It learns through a trial and error system, where it
interacts with the environment to find the best method. A state-action-reward system is
utilized, where the result of an action taken in a state is rewarded or penalized
depending on the outcome. After a training iteration it stores its Q-values in a Q-table,
where the values represent the best known expected reward for taking a given action in
a given state. It updates the table using the Temporal Difference rule
\[
Q(S, A) \leftarrow Q(S, A) + \alpha \left( R + \gamma Q(S', A') - Q(S, A) \right).
\]
For each state, the agent can either choose to
explore or to exploit. Using the Epsilon-Greedy Policy (\(\epsilon\)-greedy policy), the agent
decides whether to take the best current known action (exploit), where the agent picks
the best action with the highest Q-value based on the probability of \(1-\epsilon\). Else it will try to
find a new best possible action (explore), where the probability to explore is based
simply on the \(\epsilon\)-value. This is what allows the model to independently over time find the
best possible outcomes for each state\cite{GeeksForGeeks1RL}.

\subsection{Deep Reinforcement Learning}
Given a finite amount of actions and states, Q-learning can, therefore, learn the optimal action to take at each state to ensure the maximum total reward according to some time horizon. In 2013, Mnih\cite{DQN} proposed a variant of Q-learning called Deep Q Network (DQN), in which a neural network is used to approximate the optimal action-value function
\[
    Q^*(s,a)=\underset \pi {\text{max}} \mathbb{E}\left[ r_i+\gamma r_{t+1}+\gamma^2 r_{t+2}+\cdots \,\middle|\, s_t=s,\,a_t=a,\,\pi \right]
\]
(which is the maximum sum of rewards \(r_t\) discounted by the time horizon \(\gamma\) at each timestep \(t\), achievable by a behavior policy \(\pi=P(a|s)\), after making an observation \(s\) and taking an action \(a\)), by means of gradient decent of the loss function
\[
L_i(\theta_i)=\mathbb{E}_{(s,a,r,s')~U(D)}\left[\left(r+ \gamma \underset {a'} {\text{max}} Q(s',a';\theta^-_i)-Q(s,a;\theta)\right)^2\right]
,\]
where the quaternion \((s,a,r,s')\) represents a so-called ``experience replay'' of a past action \(a\) at a certain state \(s\), the received reward \(r\) and the next state\(s'\) following the action. With this method, then -- unlike with regular Q-learning -- an action policy for a continuous state space (like in the scenario of soccer robots in a simulation) can be learned.

\subsection{Proximal Policy Optimization (PPO)}
Proximal Policy Optimization (PPO) is a model-free reinforcement learning algorithm within the family of policy-gradient techniques. 
It enhances training stability through policy update clipping to prevent detrimental changes between iterations \cite{Schulman2017PPO}. 
PPO is broadly considered a simple, yet effective baseline for continuous control and multi-agent environments. 
As outlined in OpenAI's Spinning Up guide, PPO aims to offer a "reliable and stable improvement step for policy gradient methods" \cite{OpenAISpinningUp}. 
While PPO was originally proposed as a single-agent method, it has been widely adapted for multi-agent reinforcement learning. 
In particular, using a centralized critic with decentralized policies (often referred to as MAPPO) has become a standard approach in domains such as robotics and multi-agent games \cite{Yu2021MAPPO}. 
Its efficiency, robustness, and ease of use make PPO an attractive candidate for RoboCup SSL, wherein agents are required to act in continuous spaces and coordinate in dynamic multi-agent settings.

\subsection{Other teams}
The CMDragons team won all six games they played during the RoboCup 2015 competition. In this paper they 
describe how they used simpler algorithms to divide their robots into defense and offense subteams to suit the state 
of the game. They switched between the amount of robots depending on parameters such as ball possession, 
field region and the aggressivness of the other team. In offense, they used algorithms to both estimate the 
optimal place to move for robots without the ball as well as the best action for the robot in possession of the 
ball. In defensive situations, algorithms were used to evaluate the threats. Both first-level and second-level 
threats were computed in order to stop the robot with the ball to score directly and to stop threatening passing 
options. Using these methods, the CMDragons were able to win the competition without conceding a single goal\cite{CMDragons2015}.
Due to there succes, it can be useful to investigate if certain skills are more efficient to implement using simpler
types of algorithms instead of using AI models. 
