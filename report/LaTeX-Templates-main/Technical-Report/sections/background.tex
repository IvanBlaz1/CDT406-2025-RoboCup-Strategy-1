\section{Background}

\subsection{Behavior Trees}
Behaviour trees (BT) are a way to structure the switching between different tasks in autonomous agents. This kind 
of structure was developed for controlling NPCs (non-player characters) in games and they are both modular and reactive. 
Modular meaning that the system consists of components that are independent and reusable, e.i. the components can be 
tested individually or removed without changing the whole tree. Reactive, on the other hand, means that the system 
adapts to changes in the surrounding space and can for example change its behaviour based on what is happening. The 
structure of a BT resembles a directed rooted tree with internal nodes called control flow nodes and leaf nodes called 
execution nodes. Each connected node are most often called parent and child where the root is the node without parents. 
The execution of the tree starts with the root that sends signals with a given frequency to its children to start executing 
and these signals are called ticks. Depending on what type a child node is, it will work differently. There are typically 
four types of control flow nodes and two types of execution nodes. Starting with the control flow nodes:
\begin{itemize}
    \item \textbf{Sequence}: routs the ticks to its own children (starting from the left) and returns failure or running to its own parent when one of its children returns failure or running. Only when it has passed all its children and every child has returned success, it returns success to its own parent.
    \item \textbf{Fallback}: (also called selector) works almost like a reversed version of sequence. It routs from left to right and returns success or running as long as one child returns it and returns failure only if all children return failure. 
    \item \textbf{Parallel}: works differently from the other two, routing the ticks to all its children which means all children execute simultaneusly. The user defines how many children that needs to return succcess for the parallel node to return success to its parent. It returns failure if \textit{N-M+1} children return failure where \textit{M} is the user-defined amount and \textit{N} is the number of cildren.
    \item \textbf{Decorator}: only has one child. It is used to check that child with special conditions, such as limited execution time, or inverting the status of that child. 
\end{itemize}
For graphical representation, the sequence node is represented by a "$\rightarrow$" sign, the fallback node by a "?" sign, the parallel node by a "$\rightrightarrows$" sign, all of them with a box around the specific sign. Lastly, the decorator is represented by a rhombus.
Continuing with the execution nodes:
\begin{itemize}
    \item \textbf{Action}: executes its task and returns running while executing and success or failure when completed.
    \item \textbf{Condition}: checks a condition and returns either success (if its true) or failure (if its false) and never returns running. 
\end{itemize} 
The execution nodes are represented by a rectangle for the action node and a circle for the condition node with the specific action/condition written inside it.
In this way the flow of tasks can be controlled which makes BTs a useful tool when developing an AI system\cite{BTsinAI}. 

\subsection{Genetic Algorithm}
Genetic algorithm is a machine learning algorithm inspired by the theory of natural selection. The goal of the algorithm is to find the best possible solution to a given problem by following an evolutionary process. A population of possible solutions are tested and evaluated to measure their fitness, which is how good they are at solving the given problem. The best solutions are then brought into the next generation of solutions for further testing. The best solutions get to mate with other fitting solutions to create offspring that will have received traits from their parents. The solutions will also receive mutations to maintain genetic diversity. This entire process is then repeated until a satisfying or the best possible solution is found\cite{BiscontiniGA}.

\subsection{rcssoccer and rcssserver}
The RoboCup Soccer Simulator or rcsoccersim for short, is a tool used for research and educational purposes in regards to multiagent systems and artificial intelligence. rcssserver is a sub-repository to rcsoccersim, which contains the source code for the The RoboCup Soccer Simulator Server \cite{rcsoccersim1}.

\subsection{Reinforcement Learning}
Reinforcement learning or RL for short, is a machine learning algorithm that trains an AI model to independently make its own choices. The model learns to find the optimal policy by interacting with its environment by taking actions to find the best value function for a given policy. Through continuous training iterations it will find the best policy in regards to its value function, and in the end find the best possible solution \cite{ieeeRL1}. (BÖRJA MED STANFORD HÄRIFRÅN)
There are three main elements and an additional optional one in reinforcement learning. These are policy, reward signal, value function and optionally a model. The policy is the current behavior of the agent. It decides on what action to take depending on the current state. The reward signal, or more generally reward, is the immediate reward that the model is given for performing a certain action. The value of the reward helps the model determine if an action was good or bad. The value function helps the model determine the best choice to take currently based on what its future rewards will look like by taking said action from its current state. The value function calculates what the best possible actions will be, to maximize its reward in the long run. The fourth and optional element is the model, but they are optional. A model-based method is based around predicting the future reward without having visited each state first, and a model-free method is based around the model learning only by itself by exploring through trial and error \cite{SuttonBartoRL2}.

\subsection{Proximal Policy Optimization (PPO)}
Proximal Policy Optimization (PPO) is a model-free reinforcement learning algorithm within the family of policy-gradient techniques. 
It enhances training stability through policy update clipping to prevent detrimental changes between iterations \cite{Schulman2017PPO}. 
PPO is broadly considered a simple, yet effective baseline for continuous control and multi-agent environments. 
As outlined in OpenAI's Spinning Up guide, PPO aims to offer a "reliable and stable improvement step for policy gradient methods" \cite{OpenAISpinningUp}. 
While PPO was originally proposed as a single-agent method, it has been widely adapted for multi-agent reinforcement learning. 
In particular, using a centralized critic with decentralized policies (often referred to as MAPPO) has become a standard approach in domains such as robotics and multi-agent games \cite{Yu2021MAPPO}. 
Its efficiency, robustness, and ease of use make PPO an attractive candidate for RoboCup SSL, wherein agents are required to act in continuous spaces and coordinate in dynamic multi-agent settings.

\subsection{VMAS}
VMAS (Vectorized Multi-Agent Simulator) is a framework designed for benchmarking multi-agent reinforcement learning. It features a fully differentiable, vectorized 2D physics engine implemented in PyTorch and supports the development of custom multi-agent scenarios \cite{bettini2022vmas}. In this project, a new simulation model was created to replicate a football match in the B Division of the Small Size League. The virtual environment included a field and agent-based robots, all scaled to real-world dimensions. Core game mechanics—such as out-of-bounds, goal kicks, and corner kicks—were implemented. Additionally, fundamental skills, including shooting, passing, positioning, dribbling, creating space, and throw-ins, were incorporated.

\subsection{Other teams}
The CMDragons team won all six games they played during the RoboCup 2015 competition. In this paper they 
describe how they used simpler algorithms to divide their robots into defense and offense subteams to suit the state 
of the game. They switched between the amount of robots depending on parameters such as ball possession, 
field region and the aggressivness of the other team. In offense, they used algorithms to both estimate the 
optimal place to move for robots without the ball as well as the best action for the robot in possession of the 
ball. In defensive situations, algorithms were used to evaluate the threats. Both first-level and second-level 
threats were computed in order to stop the robot with the ball to score directly and to stop threatening passing 
options. Using these methods, the CMDragons were able to win the competition without conceding a single goal\cite{CMDragons2015}.
Due to there succes, it can be useful to investigate if certain skills are more efficient to implement using simpler
types of algorithms instead of using AI models. 
