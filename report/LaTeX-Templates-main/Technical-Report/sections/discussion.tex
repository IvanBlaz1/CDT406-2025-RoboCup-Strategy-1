\section{Discussion}
\label{section:disc}

\subsection{rcssserver}
Although rcssserver helped producing the closest thing we had to a working strategy, the general opinion regarding rcssserver is still negative. A lot of time was spent setting up basic necessities such as the UDP connection to the server, something one would think already existed as a preset for anyone to use and not have to do themselves. Not only did that take time, once that was done we had to look through the extremely limited documentation there was. The information was very vague and left a lot of things up to the readers to figure out themselves. Complications whilst running the games also appeared, as we realized we needed proper thread management to control the flow of the game which also led to time being spent solving that problem. Due to the mentioned problems, we only managed to achieve some basic training and never got to actually apply a fully finished strategy in a real game.

\subsection{Reinforcement Learning}
The results achieved by applying PPO to our VMAS RoboCup SSL setup were not as expected. Outcomes were unsatisfactory for several reasons, and further work is needed before coordinated, cooperative play can emerge.

\paragraph{Why it did not work.}
\begin{itemize}
  \item \textbf{Simulator mechanics:} Shooting and passing only worked in a very narrow pose/orientation window; small deviations often failed, so the same high-level choice produced inconsistent effects. This made the reward signal noisy and learning unstable.
  \item \textbf{Low-level skills:} High-level modes (move/dribble/shoot/pass) were mapped to hand-crafted controllers that did not always trigger predictably, so similar intents could result in weak/no kicks or unexpected ball directions.
  \item \textbf{Sparse shaping:} Rewards mostly covered progress/goals and lacked team-level signals (pass success, spacing, possession). Prior RoboCup work suggests rewarding successful, non-intercepted passes and penalising interceptions improves coordination \cite{SRC2018Team}.
  \item \textbf{Setup limits:} Minimal hyperparameter tuning, short training runs, and no role differentiation reduced stability and encouraged symmetric behaviour.
\end{itemize}

Given these issues, we did not include systematic learning curves or visualisations.

\subsection{Rule-Based System}
The success of the Rule-Based System can be attributed to its relative simplicity and deterministic nature, which enabled agents to make consistent and effective decisions. Actions were selected based on the agents' current positions on the field. While this approach may not be optimal when competing against agents powered by more advanced AI techniques, it proved highly effective against opponents that followed a predefined policy of executing the first available action. In these scenarios, the rule-based agents won every match.

In future work, once the simulation inconsistencies are addressed, implementing more systematic evaluation of agent performance would be suitable. This would include tracking and visualizing learning curves, episode rewards, and success rates for key behaviours such as scoring and passing. These evaluation methods are necessary for diagnosing problems and refining progress on agent results.
