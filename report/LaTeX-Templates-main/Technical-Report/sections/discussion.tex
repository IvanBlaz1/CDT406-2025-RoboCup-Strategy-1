\section{Discussion}
\label{section:disc}

\subsection{Reinforcement Learning}
The results we achieved with applying PPO to our VMAS Robocup SSL setup were not as expected.
In this section, I will lay out why I think the results were unsatisfactory and what we could do differently to train our agent to achieve coordinated, cooperative play and, most importantly, score goals and defend effectively.

First and foremost, we would have to create a simulator where the basic mechanics work reliably. Mostly, shooting and passing need updating.
Debugging step-by-step revealed that shooting and passing were reliant on the robot somehow getting into the perfect position and having the perfect angle.

Once this is achieved, we also have lots of other ways we could improve our agent.

\subsubsection{Changing our low level skills}
Our agent selects from hard-coded low level skills. For the agent to behave as intended, these have to be carefully selected and implemented robustly.

\subsubsection{Reward shaping}
There is a lot of room for improvement through changing our reward function.
One aspect we haven't considered yet in our reward function is passing as a reward.
Previous work shows promise in rewarding successful passes, not being intercepted, and giving a negative reward for intercepted passes
(Wei, R., Ma, W., Yu, Z., Huang, W., \& Shan, S. SRC 2018 Team Description Paper).

With our centralized approach, we can design the reward function with the state and actions of all players, not just individuals.
More strategic behaviour could therefore be achieved by maintaining good spacing, occupying key positions, and setting up opportunities for passing and teamwork.
Rewards have to be considered carefully. Our shaped rewards must still align with our primary objectives:
\begin{itemize}
    \item Scoring goals
    \item Effective defense
\end{itemize}
so that agents do not focus too much on subgoals at the expense of overall team performance.

Due to our inconsistent results stemming from our core simulation issues, we did not include systematic evaluation and visualizations of agent performance.

In future work, once the simulation inconsistencies are addressed, we plan to implement more systematic evaluation of agent performance. This would include tracking and visualizing learning curves, episode rewards, and success rates for key behaviours such as scoring and passing. These evaluation methods are necessary for diagnosing problems and refining progress on agent results.
